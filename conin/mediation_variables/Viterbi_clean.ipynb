{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27788bc4-79b7-48e6-989d-2fb4965b71fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc3a7e-11ce-4393-85b5-9597fdeb030d",
   "metadata": {},
   "source": [
    "### Split Version. OLD. See the newer version \"Viterbi_v2\" for a more general formulation that allows you to play around with various truth configurations of the constraints."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07e1d748-75f6-4344-bac2-94e1e56c61f0",
   "metadata": {},
   "source": [
    "# Model Overview.\n",
    "\n",
    "We're going with a 3-state HMM with 2-state emissions:\n",
    "\n",
    "1. Uniform transition matrix\n",
    "2. Uniform initial distribution\n",
    "3. Binary emissions with emission matrix: $\\begin{bmatrix} .8 & .2\\\\ .2 & .8\\\\ .5 & .5 \\end{bmatrix}$\n",
    "\n",
    "We're going to additionally impose the constraint that state $0$ must happen before state $1$. To make things compact, we're going use just a single mediator $M$ that tracks if state $0$ has happened yet. Then, if $X_t = 1$ but $M_t = 0$ (so $1$ happens before $0$) the constraint is violated.\n",
    "\n",
    "## Unconstrained vs Constrained\n",
    "\n",
    "By design, in the unconstrained case the optimal hidden state sequence given an observation sequence will just be a copy, That is, all hidden sequence have equal prior probability, and since state $0$ is most likely to emit $0$ and state $1$ is most likely to emit $1$, the MAP given observation sequence $O$ is just $S^* = O$.\n",
    "\n",
    "**Importantly, state $2$ will never be chosen in the unconstrained case**. However, this changes if we introduce the constraint that state $0$ must happen before $1$. Given an observation sequence like $[1,0,0,1,\\cdots]$, we will actually pick state $2$, which is more likely than state $0$ to emit $1$, until we encounter state $0$. Afterwards, the optimal hidden sequence will just be a copy of the remaining observation sequence. \n",
    "\n",
    "EDIT: If the initial sequence of $1$'s is longer than $3$ ($.5^3 \\leq .2 $), the model could be incentivized to assign a $0$-state so that it can maximize the probability remaining sequence of $1$'s by using $P(1|1) = .8$ rather than $P(1|2) = .5$. The model will assign $2$ as long as the initial sequence of $1$'s is at most 2.\n",
    "\n",
    "\n",
    "## The Augmented Model\n",
    "Let $M_t$ be the binary tracker for if state $0$ has occured in the chain yet: $M_t = M_{t-1} 1_{X_t = 0}$. Let $C_t = 1 - (1_{M_t = 0} 1_{X_t = 1})$: ie. the indicator for the negation of \"we haven't encountered state 0 yet but we're in state 1\". Enforcing state 0 to come before state 1 is equivalent to conditioning on $C_t = 1$ for all $t$. Since I'm lazy, I'm going to blow up the state space $(M,X)$ and emissions $(Y,C)$ so I can use existing code for Viterbi.\n",
    "\n",
    "\n",
    "\n",
    "### Augmented Model Ordering Convention\n",
    "Our augmented state space has 6 states corresponding to $(X,M)$, and our modified emissions matrix will be a $6 \\times 4$ matrix: 6 hiddens states and $(Y,C)$ emissions - 4 in total. We'll order the state and emission tuples by the dictionary order. \n",
    "\n",
    "1. Transition matrix. Row/cols are indexed in dictionary order: $(0,0), (1,0),\\cdots, (2,1)$. The first index is the orginal hidden state, the second is the tracker for if state $0$ has happened yet. Note the state $(0,0)$ is impossible and we set it as an absorbing state.\n",
    "2. Emission matrix. The rows of the emission matrix are indexed as $(0,0),(1,0),\\cdots,(1,1)$ where the first index is the origina emission and the second the truth value of the constraints.\n",
    "3. Observation. Since we condition on the constraints being true, we will have either $(0,1)$ or $(1,1)$. This corresponds to an index of 3 or 4 in the dictionary order.\n",
    "\n",
    "### Code Acknowledgement\n",
    "The code for the Viterbi algorith was copied from this [blog post](https://www.audiolabs-erlangen.de/resources/MIR/FMP/C5/C5S3_Viterbi.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be8b070-08f5-4b82-b4f7-2882606a506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(A, C, B, O):\n",
    "    \"\"\"Viterbi algorithm for solving the uncovering problem\n",
    "\n",
    "    Notebook: C5/C5S3_Viterbi.ipynb\n",
    "\n",
    "    Args:\n",
    "        A (np.ndarray): State transition probability matrix of dimension I x I\n",
    "        C (np.ndarray): Initial state distribution  of dimension I\n",
    "        B (np.ndarray): Output probability matrix of dimension I x K\n",
    "        O (np.ndarray): Observation sequence of length N\n",
    "\n",
    "    Returns:\n",
    "        S_opt (np.ndarray): Optimal state sequence of length N\n",
    "        D (np.ndarray): Accumulated probability matrix\n",
    "        E (np.ndarray): Backtracking matrix\n",
    "    \"\"\"\n",
    "    I = A.shape[0]  # Number of states\n",
    "    N = len(O)  # Length of observation sequence\n",
    "\n",
    "    # Initialize D and E matrices\n",
    "    D = np.zeros((I, N))\n",
    "    E = np.zeros((I, N - 1)).astype(np.int32)\n",
    "    D[:, 0] = np.multiply(C, B[:, O[0]])\n",
    "\n",
    "    # Compute D and E in a nested loop\n",
    "    for n in range(1, N):\n",
    "        for i in range(I):\n",
    "            temp_product = np.multiply(A[:, i], D[:, n - 1])\n",
    "            D[i, n] = np.max(temp_product) * B[i, O[n]]\n",
    "            E[i, n - 1] = np.argmax(temp_product)\n",
    "\n",
    "    # Backtracking\n",
    "    S_opt = np.zeros(N).astype(np.int32)\n",
    "    S_opt[-1] = np.argmax(D[:, -1])\n",
    "    for n in range(N - 2, -1, -1):\n",
    "        S_opt[n] = E[int(S_opt[n + 1]), n]\n",
    "\n",
    "    return S_opt, D, E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3761b6d6-71e5-4aec-8792-0a078c0cac6e",
   "metadata": {},
   "source": [
    "Note that a transition to $(0,0)$ is impossible, and if we transition to original state $0$ then we will always transition to augmented state $(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2abec368-b7eb-40c3-93ab-fda406e030c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.33333333, 0.33333333, 0.33333333, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.33333333, 0.33333333, 0.33333333, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.33333333, 0.33333333,\n",
       "        0.33333333],\n",
       "       [0.        , 0.        , 0.        , 0.33333333, 0.33333333,\n",
       "        0.33333333],\n",
       "       [0.        , 0.        , 0.        , 0.33333333, 0.33333333,\n",
       "        0.33333333]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmat = np.array(\n",
    "    [\n",
    "        [\n",
    "            1,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        ],  # impossible state. just set it to an absorbing one that should never occur.\n",
    "        [0, 1 / 3, 1 / 3, 1 / 3, 0, 0],\n",
    "        [0, 1 / 3, 1 / 3, 1 / 3, 0, 0],\n",
    "        [0, 0, 0, 1 / 3, 1 / 3, 1 / 3],\n",
    "        [0, 0, 0, 1 / 3, 1 / 3, 1 / 3],\n",
    "        [0, 0, 0, 1 / 3, 1 / 3, 1 / 3],\n",
    "    ]\n",
    ")\n",
    "# row/cols are indexed in dictionary order: (0,0), (1,0),...(2,1)\n",
    "#\n",
    "\n",
    "emit = np.array(\n",
    "    [\n",
    "        [0, 0, 0.8, 0.2],\n",
    "        [0.2, 0.8, 0, 0],\n",
    "        [0, 0, 0.5, 0.5],\n",
    "        [0, 0, 0.8, 0.2],\n",
    "        [0, 0, 0.2, 0.8],\n",
    "        [0, 0, 0.5, 0.5],\n",
    "    ]\n",
    ")\n",
    "tmat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04793cd0-0619-46f3-ac14-ed19b3a97f73",
   "metadata": {},
   "source": [
    "Intialize with uniform distribution over all original hidden states. Note that if we initialize to $0$, then we also initialize to $Y = 1$, which is why the initial distribution is shifted below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd6b01c-1814-4101-bf62-f5ab06d85c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.33333333, 0.33333333, 0.33333333, 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_prob = np.array([0, 1 / 3, 1 / 3, 1 / 3, 0, 0])\n",
    "init_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e6f66-c0f7-4ec3-abbe-3db28ee5952f",
   "metadata": {},
   "source": [
    "We observe the original sequence below. Since we also condition on the constraints being true, we shfit by two ie. $i \\rightarrow (i,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba581dfa-cc77-4e7a-8b4c-15e41cb1e253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_obs = [1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
    "shift_obs = np.array(og_obs) + 2\n",
    "shift_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa0871f5-3017-46b3-b16b-e5947643e8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation sequence:   O =  [3 3 3 3 2 3 2 3 3 2 3]\n",
      "Optimal Augmented Hidden State: S_aug =  [3 4 4 4 3 4 3 4 4 3 4]\n",
      "Optimal Original Hidden State: S =  [0 1 1 1 0 1 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "A = tmat\n",
    "C = init_prob\n",
    "B = emit\n",
    "O = shift_obs\n",
    "\n",
    "# O = np.array([1]).astype(np.int32)\n",
    "# O = np.array([1, 2, 0, 2, 2, 1]).astype(np.int32)\n",
    "\n",
    "# Apply Viterbi algorithm\n",
    "S_opt, D, E = viterbi(A, C, B, O)\n",
    "\n",
    "# Now convert expanded states into original states\n",
    "\n",
    "S_convert = S_opt % 3\n",
    "\n",
    "# Number of initial 1 emissions is greater than 2.\n",
    "print(\"Observation sequence:   O = \", O)\n",
    "print(\"Optimal Augmented Hidden State: S_aug = \", S_opt)\n",
    "print(\"Optimal Original Hidden State: S = \", S_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "556b2de2-69ff-491d-93b3-87dfaf1ba394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2, 3, 2, 3, 3, 2, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_obs = [1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
    "shift_obs = np.array(og_obs) + 2\n",
    "shift_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5790d5a3-589b-4f50-9b04-a1e9605cd5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation sequence:   O =  [3 3 2 3 2 3 3 2 3]\n",
      "Optimal Augmented Hidden State: S_aug =  [2 2 3 4 3 4 4 3 4]\n",
      "Optimal Original Hidden State: S =  [2 2 0 1 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "A = tmat\n",
    "C = init_prob\n",
    "B = emit\n",
    "O = shift_obs\n",
    "\n",
    "# O = np.array([1]).astype(np.int32)\n",
    "# O = np.array([1, 2, 0, 2, 2, 1]).astype(np.int32)\n",
    "\n",
    "# Apply Viterbi algorithm\n",
    "S_opt, D, E = viterbi(A, C, B, O)\n",
    "\n",
    "# Now convert expanded states into original states\n",
    "\n",
    "S_convert = S_opt % 3\n",
    "\n",
    "# Number of initial 1 emissions is greater than 2.\n",
    "print(\"Observation sequence:   O = \", O)\n",
    "print(\"Optimal Augmented Hidden State: S_aug = \", S_opt)\n",
    "print(\"Optimal Original Hidden State: S = \", S_convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192dd02d-7e72-435f-a65e-fe35372ccef3",
   "metadata": {},
   "source": [
    "Here is a weakness in the current formulation. Let say we now want to do inference with the knowledge that the constraint is false: state 1 happens before state 0. From our setup, this is equivalent to conditioning on the event that at least one $C_t =0$. But this is not so easy since it doesn't translate into a single assignment to the augmented emissions $C_t$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ccfff-c8ef-4e5f-9a08-9b812687b730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clio(stable)",
   "language": "python",
   "name": "clio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
