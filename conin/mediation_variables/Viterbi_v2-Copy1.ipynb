{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5a243af-db56-44e7-af89-1782eec95667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import warnings\n",
    "# from munch import Munch\n",
    "# import itertools\n",
    "# from mv_Viterbi import mv_Viterbi\n",
    "# from cst_aggregate import cst_aggregate\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from munch import Munch\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314f0207-d5af-4d20-81d3-777fb5f0557e",
   "metadata": {},
   "source": [
    "### Create the HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c49caa1-f703-4878-9114-fc57cb03ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = ['pro','ex1','ex2','int','dis','enh']\n",
    "emit_states = ['A','T', 'C', 'G']\n",
    "hidden_size, emit_size = len(hidden_states), len(emit_states)\n",
    "\n",
    "hmm_mat = np.array([\n",
    "    [.6,.1,.1,.1,.1,0], #promoter\n",
    "    [0,.4,.2,.2,.1,.1], #exon1\n",
    "    [.0,.1,.6,.1,.1,.1], #exon2\n",
    "    [.2,.1,.1,.5,0,.1], #intron\n",
    "    [0,1/3, 1/3, 0,1/3,0], #disease\n",
    "    [0,.25,.25,.25,0,.25] #enhancer\n",
    "])\n",
    "\n",
    "emit_mat = np.array([ #\n",
    "    [.1,.1,.4,.4], #CG rich promoter\n",
    "    [.2,.2,.5,.1], #Exon 1 favors C\n",
    "    [.5,.1,.2,.2], #Exon 2 favors A\n",
    "    [.25,.25,.25,.25], #Intron \n",
    "    [.4,.1,.4,.1], #Disease favors AC\n",
    "    [.4,.4,.1,.1] #AT rich enhancer\n",
    "])\n",
    "\n",
    "init_vec = np.array(\n",
    "    [.2,0,0,.8,0,0]\n",
    ")\n",
    "\n",
    "hmm_transition = {}\n",
    "for i in range(hidden_size):\n",
    "    for j in range(hidden_size):\n",
    "        hmm_transition[hidden_states[i],hidden_states[j]] = hmm_mat[i,j].item()\n",
    "\n",
    "hmm_emit = {}\n",
    "for i in range(hidden_size):\n",
    "    for j in range(emit_size):\n",
    "        hmm_emit[hidden_states[i],emit_states[j]] = emit_mat[i,j].item()\n",
    "        \n",
    "hmm_startprob = {}\n",
    "for i in range(hidden_size):\n",
    "    hmm_startprob[hidden_states[i]] = init_vec[i]\n",
    "\n",
    "hmm = Munch(states = hidden_states, emits = emit_states, tprob = hmm_transition, eprob = hmm_emit, initprob = hmm_startprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63112492-5421-4a05-8c8c-2c6ff6ac5e63",
   "metadata": {},
   "source": [
    "### Stay > = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bf7bfa6-e92d-436c-ac62-abea9de469d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cst_params(cst, hidden_states, dtype = torch.float16, device = 'cpu'):\n",
    "    m_states = cst.m_states\n",
    "    init = cst.init_fun\n",
    "    upd = cst.update_fun\n",
    "    eval = cst.eval_fun\n",
    "\n",
    "    #returns a (k,s,r) array. k is current hideen. r,s are present/past mediation.\n",
    "    upd_mat = torch.tensor([[[upd(k,s, r) for r in m_states] for s in m_states] for k in hidden_states], dtype = dtype, device = device)\n",
    "\n",
    "    #returns a (k,r) array. k,r are current hidden/mediation states\n",
    "    init_mat = torch.tensor([[init(k,r) for r in m_states] for k in hidden_states], dtype = dtype, device = device)\n",
    "\n",
    "    #return (k,r) array for terminal emission.\n",
    "    eval_mat = torch.tensor([[eval(k,r) for r in m_states] for k in hidden_states], dtype = dtype, device = device)\n",
    "\n",
    "    return init_mat, upd_mat, eval_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cefac4ef-bb55-41a3-bb1d-748192162043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_fun(k , r_past, r):\n",
    "    '''\n",
    "    r = hidden_states x [1,2,3,4,5]\n",
    "    '''\n",
    "    prev, count = r_past #r is a tuple\n",
    "    if k == prev:\n",
    "        new_count = count + 1\n",
    "    else:\n",
    "        new_count = 1\n",
    "        \n",
    "    consistency = (count == 5) or (k == prev) #0 if transition to new state without staying 3\n",
    "\n",
    "    return (r == (k,new_count)) and consistency\n",
    "\n",
    "def init_fun(k, r):\n",
    "    '''\n",
    "    initial \"prob\" of r = (m1,m2) from k. is just indicator\n",
    "    '''\n",
    "\n",
    "    return r == (k,1)\n",
    "\n",
    "    \n",
    "def eval_fun(k, r):\n",
    "    return True\n",
    "\n",
    "m_states = list(itertools.product(hidden_states, list(range(1,6))))\n",
    "\n",
    "stay_cst = Munch(update_fun = update_fun, init_fun = init_fun, eval_fun = eval_fun, m_states = m_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57af1a3a-9b2b-4b96-9e0b-9780506a3f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_fun('enh',('pro',4), ('enh',1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9999de93-d7ee-4f8a-a288-aa58a17ca16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = create_cst_params(stay_cst, hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54497b52-da17-40a3-8f69-49b3409925b7",
   "metadata": {},
   "source": [
    "### Promoter Must Occur in First 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e8868ef-b179-4ac6-a5ad-b2e98e8ce822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_fun(k , r_past, r):\n",
    "    '''\n",
    "    r = Boolean\n",
    "    tracks if 'pro' has occured yet or not\n",
    "    '''        \n",
    "\n",
    "    return r == (r_past or (k == 'pro')) \n",
    "\n",
    "def init_fun(k, r):\n",
    "\n",
    "    return r == (k == 'pro')\n",
    "\n",
    "def eval_fun(k,r):\n",
    "    return r == True\n",
    "\n",
    "m_states = [True,False]\n",
    "\n",
    "promoter_cst = Munch(update_fun = update_fun, init_fun = init_fun, eval_fun = eval_fun, m_states = m_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e81d8e8-d03e-466e-bac7-2a647dcea231",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = create_cst_params(promoter_cst, hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9540c9a-43cc-4f03-8b44-cbd04c2aaa4e",
   "metadata": {},
   "source": [
    "#### Visit Dis Exactly Once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81f1acc6-9575-4cc3-be9e-f1ae6f51aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_fun(k , r_past, r):\n",
    "    '''\n",
    "    r = [0,1,2]\n",
    "    tracks if 'pro' has occured yet or not\n",
    "    '''\n",
    "    if k == 'dis':\n",
    "        count = max(r + 1, 2)\n",
    "        \n",
    "    else:\n",
    "        count = r_past\n",
    "    \n",
    "    return r == count \n",
    "\n",
    "def init_fun(k, r):\n",
    "\n",
    "    return r == int(k == 'dis')\n",
    "\n",
    "def eval_fun(r, sat):\n",
    "    return r == 1 #must be exactly 1.\n",
    "\n",
    "m_states = list(range(3))\n",
    "\n",
    "disvisit_cst = Munch(update_fun = update_fun, init_fun = init_fun, eval_fun = eval_fun, m_states = m_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48131627-56ad-4a8e-9bbf-cd168eb8382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = create_cst_params(disvisit_cst, hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8a6e5b-8e28-4bfc-8cea-0e2c6ddaf35b",
   "metadata": {},
   "source": [
    "### Promoter < Disease < Enhancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ddddd9a-8692-4e5c-93cc-b151690edb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_fun(k , r_past, r):\n",
    "    '''\n",
    "    r = Boolean_pro x Bool_dis x Bool_enh\n",
    "    trcks that they occur in sequence\n",
    "    '''\n",
    "    occur_pro, occur_dis, occur_enh = r_past\n",
    "    consist = True\n",
    "    \n",
    "    pro_new = (k == 'pro' or occur_pro)\n",
    "    dis_new = (k == 'dis' or occur_dis)\n",
    "    enh_new = (k == 'pro' or occur_enh)\n",
    "\n",
    "    if k == 'dis':\n",
    "        consist = occur_pro\n",
    "\n",
    "    if k == 'enh':\n",
    "        consist = occur_dis \n",
    "\n",
    "    return (r == (pro_new, dis_new,enh_new)) and consist\n",
    "\n",
    "def init_fun(k, r):\n",
    "\n",
    "    return r == ( k == 'pro', k == 'dis', k == 'enh')\n",
    "\n",
    "def eval_fun(k,r):\n",
    "    return True\n",
    "\n",
    "m_states = list(itertools.product([True, False], repeat=3))\n",
    "\n",
    "pde_cst = Munch(update_fun = update_fun, init_fun = init_fun, eval_fun = eval_fun, m_states = m_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "615560dd-41f4-45fd-9fc1-daeffead5876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a,b,c \u001b[38;5;241m=\u001b[39m create_cst_params(pde_cst, \u001b[43mhidden_states\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hidden_states' is not defined"
     ]
    }
   ],
   "source": [
    "a,b,c = create_cst_params(pde_cst, hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1466b5a0-71da-4114-8116-b2e021091af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cst_list = [disvisit_cst, pde_cst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43229745-0333-4e8c-a2f7-9d3364c48280",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params, testcst_params = convertTensor_list(hmm, cst_list, dtype = torch.float32, device = 'cpu', hmm_params = None, return_ix = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50952bc9-97d2-4a9b-b2e1-e6e75037eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cst_params(cst, hidden_states, dtype = torch.float16, device = 'cpu'):\n",
    "    m_states = cst.m_states\n",
    "    init = cst.init_fun\n",
    "    upd = cst.update_fun\n",
    "    eval = cst.eval_fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a33b075-8ad7-47ff-8604-d5ea8debb374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cst_params(cst, hidden_states, dtype = torch.float16, device = 'cpu'):\n",
    "    m_states = cst.m_states\n",
    "    init = cst.init_fun\n",
    "    upd = cst.update_fun\n",
    "    eval = cst.eval_fun\n",
    "\n",
    "    #returns a (k,s,r) array. k is current hideen. r,s are present/past mediation.\n",
    "    upd_mat = torch.tensor([[[upd(k,s, r) for r in m_states] for s in m_states] for k in hidden_states], dtype = dtype, device = device)\n",
    "\n",
    "    #returns a (k,r) array. k,r are current hidden/mediation states\n",
    "    init_mat = torch.tensor([[init(k,r) for r in m_states] for k in hidden_states], dtype = dtype, device = device)\n",
    "\n",
    "    #return (k,r) array for terminal emission.\n",
    "    eval_mat = torch.tensor([[eval(k,r) for r in m_states] for k in hidden_states], dtype = dtype, device = device)\n",
    "\n",
    "    return init_mat, upd_mat, eval_mat\n",
    "\n",
    "def convertTensor_list(hmm, cst_list, dtype = torch.float16, device = 'cpu', hmm_params = None, return_ix = False):\n",
    "    '''\n",
    "    cst_list is a list of the individual csts.\n",
    "    '''\n",
    "    #Initialize and convert all quantities  to np.arrays\n",
    "    hmm = copy.deepcopy(hmm)\n",
    "    K = len(hmm.states)\n",
    "    \n",
    "    state_ix = {s: i for i, s in enumerate(hmm.states)}\n",
    "    \n",
    "    #Compute the hmm parameters if not provided\n",
    "    if hmm_params is None:\n",
    "        tmat = torch.zeros((K,K), dtype=dtype ).to(device)\n",
    "        init_prob = torch.zeros(K, dtype=dtype ).to(device)\n",
    "    \n",
    "        for i in hmm.states:\n",
    "            init_prob[state_ix[i]] = hmm.initprob[i]\n",
    "            for j in hmm.states:\n",
    "                tmat[state_ix[i],state_ix[j]] = hmm.tprob[i,j]\n",
    "    \n",
    "        hmm_params = [tmat, init_prob]\n",
    "    \n",
    "    #Compute the cst parameters \n",
    "    init_list = []\n",
    "    eval_list = []\n",
    "    upd_list = []\n",
    "    dims_list = []\n",
    "    cst_ix = 0\n",
    "    C = len(cst_list)\n",
    "\n",
    "    #indices are (hidden, c_1,....,c_C, hidden, c_1,....,c_C) are augmented messages\n",
    "    for cst in cst_list:\n",
    "        cst = copy.deepcopy(cst)\n",
    "        init_mat, upd_mat, eval_mat = create_cst_params(cst, hidden_states, dtype = dtype, device = device)\n",
    "        init_list += [init_mat,[0,cst_ix + 1]]\n",
    "        eval_list += [eval_mat, [0, cst_ix + 1]]\n",
    "        upd_list += [upd_mat, [0, cst_ix + 1, C + 1, cst_ix + C + 2]]\n",
    "        dims_list.append(len(cst.m_states))\n",
    "        cst_ix += 1\n",
    "                \n",
    "    cst_params = [dims_list, init_list,upd_list,upd_list]\n",
    "\n",
    "    if return_ix:\n",
    "        return hmm_params, cst_params, state_ix\n",
    "    return hmm_params, cst_params \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b2561ca-137d-4c46-ba60-08adbfe71f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_emitweights(obs,hmm, time_hom = True):\n",
    "    '''\n",
    "    Separately handles the computation of the \n",
    "    '''\n",
    "    hmm = copy.deepcopy(hmm) #protect again in place modification\n",
    "    T = len(obs)\n",
    "    K = len(hmm.states)\n",
    "    #Compute emissions weights for easier access\n",
    "    emit_weights = np.zeros((T,K))\n",
    "    for t in range(T):\n",
    "        if time_hom:\n",
    "            emit_weights[t] = np.array([hmm.eprob[k,obs[t]] for k in hmm.states])\n",
    "        else:\n",
    "            emit_weights[t] = np.array([hmm.eprob[t,k,obs[t]] for k in hmm.states])\n",
    "    return emit_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b71101-fcee-4dba-aa08-3ff61f7141a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi_torch_list(hmm, hmm_params, cst_list, obs, dtype = torch.float16,  device = 'cpu', debug = False, num_cst = 0):\n",
    "    '''\n",
    "    more optimized torch implementation of Viterbi. The constraint all evolve independently (ie. factorial), so no need to create a big U_krjs matrix. Instead, just multiply along given dim. Still require computing V_{krjs}, but this should help.\n",
    "    For numerica underflow, we normalize the value at each time. Also, we add a small constant num_cst when normalizing.\n",
    "    '''\n",
    "    hmm = copy.deepcopy(hmm) #protect again in place modification\n",
    "    #Generate emit_weights:\n",
    "    emit_weights = compute_emitweights(obs, hmm, time_hom)\n",
    "    emit_weights = torch.from_numpy(emit_weights).type(dtype).to(device)\n",
    "\n",
    "    #Generate hmm,cst params:\n",
    "    hmm_params, cst_params_list, state_ix = convertTensor_list(hmm,cst_list, sat, dtype = dtype, \\\n",
    "                                                               device = device, return_ix = True)   \n",
    "    tmat, init_prob = hmm_params\n",
    "    dims_list, init_ind_list,final_ind_list,ind_list = cst_params_list\n",
    "\n",
    "    \n",
    "    #Viterbi\n",
    "    T = emit_weights.shape[0]\n",
    "    K = tmat.shape[0]\n",
    "    C = len(dims_list)\n",
    "    \n",
    "    val = torch.empty((T,K) + tuple(dims_list), device = 'cpu')\n",
    "    ix_tracker = torch.empty((T,K) + tuple(dims_list), device = 'cpu') #will store flattened indices\n",
    "    \n",
    "    kr_indices = list(range(C+1))\n",
    "    kr_shape = (K,) + tuple(dims_list)\n",
    "    js_indices = [k + C + 1 for k in kr_indices]\n",
    "\n",
    "    #Forward pass\n",
    "    # V = torch.einsum('k,k,kr -> kr', init_prob, emit_weights[0], init_ind)\n",
    "    V = torch.einsum(emit_weights[0], [0], init_prob, [0], *init_ind_list, kr_indices)\n",
    "    V = V/(V.max() + num_cst) #normalize for numerical stability\n",
    "    val[0] = V.cpu()\n",
    "    for t in range(1,T):\n",
    "        # V = torch.einsum('js,jk,krjs -> krjs',val[t-1],tmat,ind)\n",
    "        V = torch.einsum(val[t-1].to(device), js_indices, tmat, [C+1,0], *ind_list, list(range(2*C + 2)))\n",
    "        V = V.reshape(tuple(kr_shape) + (-1,))\n",
    "        V = V/(V.max() + num_cst)\n",
    "        max_ix = torch.argmax(V, axis = -1, keepdims = True)\n",
    "        ix_tracker[t-1] = max_ix.squeeze()\n",
    "        V = torch.take_along_dim(V, max_ix, axis=-1).squeeze()\n",
    "        if t == T:\n",
    "            # val[t] = torch.einsum('k,kr,kr -> kr',emit_weights[t],final_ind,V)\n",
    "            val[t] = torch.einsum(emit_weights[t],[0], V, kr_indices,*final_ind_list, kr_indices).cpu()\n",
    "        else:\n",
    "            # val[t] = torch.einsum('k,kr -> kr', emit_weights[t],V)\n",
    "            val[t] = torch.einsum(emit_weights[t],[0], V, kr_indices, kr_indices).cpu()\n",
    "        \n",
    "    state_ix = {v:k for k,v in state_ix.items()}\n",
    "    #Backward pass\n",
    "    opt_augstateix_list = []\n",
    "    max_ix = int(torch.argmax(val[T-1]).item())\n",
    "    unravel_max_ix = np.unravel_index(max_ix, kr_shape)\n",
    "    opt_augstateix_list =  [np.array(unravel_max_ix).tolist()] + opt_augstateix_list\n",
    "    \n",
    "    ix_tracker = ix_tracker.reshape(T,-1) #flatten again for easier indexing    \n",
    "    \n",
    "    for t in range(T-1):\n",
    "        max_ix =  int(ix_tracker[T-2-t,max_ix].item())\n",
    "        unravel_max_ix = np.unravel_index(max_ix, kr_shape)\n",
    "        opt_augstateix_list =  [np.array(unravel_max_ix).tolist()] + opt_augstateix_list\n",
    "\n",
    "    opt_state_list = [state_ix[k[0]] for k in opt_augstateix_list]\n",
    "    if debug:\n",
    "        return opt_state_list, opt_augstateix_list, val, ix_tracker\n",
    "    return opt_state_list, opt_augstateix_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365da95a-c61a-4368-b6f3-10d558bd0e89",
   "metadata": {},
   "source": [
    "### Inference when the Constraint is Satisfied\n",
    "\n",
    "Here, we constrain $C=1$: $a$ must happen before $c$. As predicted, when encountering an initial sequence of $C$'s, our model choose $b$ since $c$ is not allowed and $b$ has a higher chance of emitting $C$. Provided the initial number of $C$'s is at most 2, we'll see this behavior. We can increase the admissable length of $b$'s by decreasing the emission probabilities $a,A$ and $c,C$ if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "538fb16b-9480-47f3-a9a8-3b41238063ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = ['C','A','C','A','C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c833975-3d43-4d05-954e-68683594ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_aug, opt_state = mv_Viterbi(obs, hmm, precedence_cst, sat = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09697322-d13c-486b-a8b8-918b6c397d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'a', 'c', 'a', 'c']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c5579-cda8-4dd4-b96b-36fc7f51967a",
   "metadata": {},
   "source": [
    "### Inference when the COnstraint is NOT Satsified\n",
    "\n",
    "Now, we observe $C= 0$: that the constrain is not satisifed. It's logical negation is just that $c$ happens before $a$, and the inferene situation is symmetric. We see that encountering a small initial sequence of $A$'s makes us choose $b$ for the same reasons as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "520adc93-6bb5-4539-8544-b898d42a82a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = ['A','A','C','A','C','A','C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dc4716b-e43d-43c4-984e-c81ed6aea3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_aug, opt_state = mv_Viterbi(obs, hmm, precedence_cst, sat = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23d70283-173c-4f77-8a54-d16ab4171f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'b', 'c', 'a', 'c', 'a', 'c']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c430ca-59ab-402b-a386-ce791e27f2a6",
   "metadata": {},
   "source": [
    "# Occurence Constraint\n",
    "\n",
    "Now, we create anothe constraint class that enforce that state $b$ must be visited at some point. This is equivalent to replacing just one of $a$ or $c$ in the unconstrained MAP with $b$, at any time point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f3ed901-d1fe-40c7-b95e-43b34c288b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_fun2(r,k , r_past):\n",
    "    '''\n",
    "    m1 = = tau_b or b . tracks if b has occured\n",
    "    '''\n",
    "    m1 = (k == 'b') or r_past[0]\n",
    "\n",
    "    return int(r == (m1,))\n",
    "\n",
    "def init_fun2(k, r):\n",
    "    '''\n",
    "    initial \"prob\" of r = m1,m2 from k. is just indicator\n",
    "    '''\n",
    "    m1 = k == 'b'\n",
    "\n",
    "    return int(r == (m1,))\n",
    "    \n",
    "def cst_fun2(r, sat):\n",
    "    '''\n",
    "    Constraint is a boolean emissions of the final auxillary state. In this case\n",
    "    '''\n",
    "    \n",
    "    return int(r[0]  == sat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "490d64d7-85d3-4e56-9087-45ca0372d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurence_cst = Munch(name = 'b must occur', aux_size = 1, update_fun = update_fun2, init_fun = init_fun2, cst_fun = cst_fun2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e432dda-f03a-4b8c-b8de-e580d4b6bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = ['C','C','A','C','A','C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb7f0c6a-5e95-40b2-8314-3dc46ea1a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_aug, opt_state = mv_Viterbi(obs, hmm, occurence_cst, sat = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fe66045-ea85-4921-8a7e-8156290d89b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'c', 'a', 'c', 'a', 'c']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdbe9c4-ee62-42c5-95dd-419099ca2102",
   "metadata": {},
   "source": [
    "## Occurent Constraint is False\n",
    "\n",
    "If we condition on the constraint being false, this is equivalent to \"$b$ is never visited\". Since unconstrained inference will never return $b$, setting the constriant to be False will give the same answer as unconstrained inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e82a6e3-f11b-4dbd-8d0f-443e83a35a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = ['C','C','A','C','A','C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93358122-24fc-43a3-836a-a9233af1aeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c', 'c', 'a', 'c', 'a', 'c']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_aug, opt_state = mv_Viterbi(obs, hmm, occurence_cst, sat = False)\n",
    "opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173fdfa0-36f8-42c7-99a0-f903005b3dbd",
   "metadata": {},
   "source": [
    "# Conditioning on Multiple Constraints and Their Values\n",
    "\n",
    "Now, we'll introduce both the precendence constraint \"$a$ happens before $c$\" and \"$b$ must happen at some point\" into our model. Again, these are modeled as binary emissions, so we can play with their truth configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff76b0c7-8eb1-4b01-9e85-239552fceeef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a occurs before c', 'b must occur']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cst_list = [precedence_cst,occurence_cst]\n",
    "combined_cst = cst_aggregate(cst_list)\n",
    "combined_cst.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03136381-8ed6-454b-a661-85e6166aa130",
   "metadata": {},
   "source": [
    "# Both True\n",
    "\n",
    "First, we assume both constraints are true. Note that the below observation sequence is chosen so that the precendence constraint already makes $b$ appear first, so the occurence constraint is satsified automatically. Therefore, the answer should be the same as just conditioning on the precendence constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7e31019-8ef4-4124-b4f5-b10df099fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = ['C','C','A','C','A','C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b04a6df1-08b2-4c52-9eb3-bc1bba2c7f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_aug, opt_state = mv_Viterbi(obs, hmm, combined_cst, sat = (True,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3d7509f-1a47-4a5a-8560-3497d5ae6e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'b', 'a', 'c', 'a', 'c']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b39aa0-37a1-45b0-8d12-d049e4106dbe",
   "metadata": {},
   "source": [
    "### Precendence True, Occurence False\n",
    "\n",
    "Now here's an interesting scenario. The occurence constraint being unsatisfied is equivalent to $b$ never occuring. Now, when the precendence constraint kicks in, we can only choose $a$ or $c$. This means that any initial sequence of $C$ emissions is forced to return $a$, as opposed to $b$ if we were just enforcing the precendence constraint by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59a103e9-98af-4776-b92c-077c8791892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = ['C','C','A','C','A','C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84a86266-6199-4767-8e62-7ffee0911dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'c', 'a', 'c', 'a', 'c']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_aug, opt_state = mv_Viterbi(obs, hmm, combined_cst, sat = (True,False))\n",
    "opt_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conin",
   "language": "python",
   "name": "conin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
